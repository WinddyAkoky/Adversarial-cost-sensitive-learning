{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from utils import my_fgsm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NORMALIZE = False\n",
    "\n",
    "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_home = 'F:\\\\work'\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.CIFAR10(root=os.path.join(data_home, 'dataset/CIFAR10'), train=True, download=True, transform=train_transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root=os.path.join(data_home, 'dataset/CIFAR10'), train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_matric(i_label):\n",
    "    C = torch.ones(10,10)\n",
    "    C[i_label,:] = 2\n",
    "    C[:,i_label] = 2\n",
    "    C = C - torch.diag(C.diag())\n",
    "    return C\n",
    "\n",
    "class Loss_cost_sensitive(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(Loss_cost_sensitive, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, data, target, c):\n",
    "        \n",
    "        l1 = F.cross_entropy(data, target, reduction='mean')\n",
    "        p = F.softmax(data, 1)\n",
    "        \n",
    "        cost_sentive = c[:,target]\n",
    "        cost_sentive = cost_sentive.T\n",
    "        l2 = p.mul(cost_sentive)\n",
    "        l2 = l2.sum(1).mean()\n",
    "        \n",
    "        conv_weight = self.model.conv1.weight\n",
    "        loss_x = torch.norm(conv_weight, p=1) - torch.norm(conv_weight, p=2)\n",
    "#         loss_x = -0.1*torch.norm(conv_weight, p=1) - torch.norm(conv_weight, p=2)\n",
    "#         return l1\n",
    "        return l1+l2 + loss_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一般对抗训练的模型\n",
    "model_ADV = LeNet()\n",
    "\n",
    "model_ADV.load_state_dict(torch.load('../model/Lenet_CIFAR.pt'))\n",
    "\n",
    "# 正常模型\n",
    "model_normal = LeNet()\n",
    "model_normal.load_state_dict(torch.load('../model/Lenet_CIFAR.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protect label: 0\n",
      "load cost matric: \n",
      "tensor([[0., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [2., 1., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [2., 1., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [2., 1., 1., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [2., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [2., 1., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
      "        [2., 1., 1., 1., 1., 1., 1., 0., 1., 1.],\n",
      "        [2., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
      "        [2., 1., 1., 1., 1., 1., 1., 1., 1., 0.]])\n",
      "开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50000|50000, loss:3488.5374925136566epoch: 0, test correct: 0.2494\n",
      " 50000|50000, loss:2358.2944946289062epoch: 1, test correct: 0.3232\n",
      " 50000|50000, loss:2202.1200180053717epoch: 2, test correct: 0.3484\n",
      " 50000|50000, loss:2098.3165965080267epoch: 3, test correct: 0.3799\n",
      " 50000|50000, loss:2042.5490832328796epoch: 4, test correct: 0.3829\n",
      " 50000|50000, loss:2006.4566664695745epoch: 5, test correct: 0.3979\n",
      " 50000|50000, loss:1974.7070100307465epoch: 6, test correct: 0.4073\n",
      " 50000|50000, loss:1950.2349448204046epoch: 7, test correct: 0.418\n",
      " 50000|50000, loss:1925.6282587051392epoch: 8, test correct: 0.4188\n",
      " 50000|50000, loss:1905.8589544296265epoch: 9, test correct: 0.4224\n",
      " 50000|50000, loss:1887.3484418392181epoch: 10, test correct: 0.4314\n",
      " 50000|50000, loss:1874.6392643451696epoch: 11, test correct: 0.4345\n",
      " 50000|50000, loss:1859.4913933277134epoch: 12, test correct: 0.4335\n",
      " 50000|50000, loss:1847.5138382911682epoch: 13, test correct: 0.4384\n",
      " 50000|50000, loss:1838.1556876897812epoch: 14, test correct: 0.4414\n",
      " 50000|50000, loss:1826.3019218444824epoch: 15, test correct: 0.4434\n",
      " 50000|50000, loss:1820.3293526172638epoch: 16, test correct: 0.4436\n",
      " 50000|50000, loss:1810.3968505859375epoch: 17, test correct: 0.4438\n",
      " 50000|50000, loss:1804.8115284442902epoch: 18, test correct: 0.4493\n",
      " 50000|50000, loss:1799.2449444532394epoch: 19, test correct: 0.4509\n",
      "对 model_cost_sensitive 的评估\n",
      " 1000\n",
      " 0 correct: 0.174\n",
      " 1000\n",
      " 1 correct: 0.273\n",
      " 1000\n",
      " 2 correct: 0.003\n",
      " 1000\n",
      " 3 correct: 0.014\n",
      " 1000\n",
      " 4 correct: 0.041\n",
      " 1000\n",
      " 5 correct: 0.063\n",
      " 1000\n",
      " 6 correct: 0.029\n",
      " 1000\n",
      " 7 correct: 0.189\n",
      " 1000\n",
      " 8 correct: 0.04\n",
      " 1000\n",
      " 9 correct: 0.361\n",
      "对 model_normal的评估\n",
      " 1000\n",
      " 0 correct: 0.0\n",
      " 1000\n",
      " 1 correct: 0.002\n",
      " 1000\n",
      " 2 correct: 0.0\n",
      " 1000\n",
      " 3 correct: 0.001\n",
      " 1000\n",
      " 4 correct: 0.002\n",
      " 1000\n",
      " 5 correct: 0.001\n",
      " 1000\n",
      " 6 correct: 0.015\n",
      " 1000\n",
      " 7 correct: 0.003\n",
      " 1000\n",
      " 8 correct: 0.0\n",
      " 1000\n",
      " 9 correct: 0.464\n",
      "对 model_adv的评估\n",
      " 1000\n",
      " 0 correct: 0.0\n",
      " 1000\n",
      " 1 correct: 0.002\n",
      " 1000\n",
      " 2 correct: 0.0\n",
      " 1000\n",
      " 3 correct: 0.001\n",
      " 1000\n",
      " 4 correct: 0.002\n",
      " 1000\n",
      " 5 correct: 0.001\n",
      " 1000\n",
      " 6 correct: 0.015\n",
      " 1000\n",
      " 7 correct: 0.003\n",
      " 1000\n",
      " 8 correct: 0.0\n",
      " 1000\n",
      " 9 correct: 0.464\n",
      "protect label: 1\n",
      "load cost matric: \n",
      "tensor([[0., 2., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [2., 0., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [1., 2., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 2., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 2., 1., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 2., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 2., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
      "        [1., 2., 1., 1., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 2., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
      "        [1., 2., 1., 1., 1., 1., 1., 1., 1., 0.]])\n",
      "开始训练\n",
      " 50000|50000, loss:3544.1518511772156epoch: 0, test correct: 0.2914\n",
      " 50000|50000, loss:2387.3219740390778epoch: 1, test correct: 0.2884\n",
      " 50000|50000, loss:2278.0556447505957epoch: 2, test correct: 0.3513\n",
      " 50000|50000, loss:2189.8627510070828epoch: 3, test correct: 0.3755\n",
      " 50000|50000, loss:2123.7537107467653epoch: 4, test correct: 0.3728\n",
      " 50000|50000, loss:2061.1185426712036epoch: 5, test correct: 0.3796\n",
      " 50000|50000, loss:2018.7266230583192epoch: 6, test correct: 0.3772\n",
      " 50000|50000, loss:1990.8472940921783epoch: 7, test correct: 0.4217\n",
      " 50000|50000, loss:1965.5497181415558epoch: 8, test correct: 0.4346\n",
      " 50000|50000, loss:1938.4430141448975epoch: 9, test correct: 0.4376\n",
      " 50000|50000, loss:1920.9057912826538epoch: 10, test correct: 0.4347\n",
      " 50000|50000, loss:1900.4538854360583epoch: 11, test correct: 0.4522\n",
      " 50000|50000, loss:1888.0894404649734epoch: 12, test correct: 0.45\n",
      " 50000|50000, loss:1873.1935760974884epoch: 13, test correct: 0.4542\n",
      " 50000|50000, loss:1858.7642946243286epoch: 14, test correct: 0.4599\n",
      " 50000|50000, loss:1850.5423066616058epoch: 15, test correct: 0.4611\n",
      " 40320|50000, loss:1479.1590547561646"
     ]
    }
   ],
   "source": [
    "# 记录结果\n",
    "results_infos = {}\n",
    "\n",
    "# 先读取未经过对抗训练的模型\n",
    "# 在进行对抗训练\n",
    "\n",
    "# 参数\n",
    "epsilon = 0.3\n",
    "if NORMALIZE:\n",
    "    model_path = ''\n",
    "else:\n",
    "    model_path = '../model/Lenet_CIFAR.pt'\n",
    "\n",
    "# 循环 对每一个类分别进行保护\n",
    "for i_label in range(10):\n",
    "    # 读取预训练模型\n",
    "    model_cost_sensitive = LeNet()\n",
    "#     model_cost_sensitive.load_state_dict(torch.load(model_path))\n",
    "#     print('load model for initialization: {}'.format(model_path))\n",
    "    model_cost_sensitive = model_cost_sensitive.to(DEVICE)\n",
    "    \n",
    "    criterion_cost_sensitive = Loss_cost_sensitive(model_cost_sensitive)\n",
    "#     criterion_cost_sensitive = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model_cost_sensitive.parameters(), lr=0.001)\n",
    "    schedule = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.9)\n",
    "    \n",
    "    C = get_cost_matric(i_label)\n",
    "    C = C.to(DEVICE)\n",
    "    print('protect label: {}'.format(i_label))\n",
    "    print('load cost matric: ')\n",
    "    print(C)\n",
    "    \n",
    "    LABEL = 'Protect Label ' + str(i_label)\n",
    "    \n",
    "    # 开始训练\n",
    "    print('开始训练')\n",
    "    for epoch in range(20):\n",
    "        count = 0\n",
    "        loss_sum = 0\n",
    "        model_cost_sensitive.train()\n",
    "        schedule.step(epoch)\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_cost_sensitive(data)    \n",
    "            loss = criterion_cost_sensitive(output, target, C)\n",
    "#             loss = criterion_cost_sensitive(output,target)\n",
    "            loss_sum += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            count += len(data)\n",
    "            print('\\r {}|{}, loss:{}'.format(count, len(train_loader.dataset), loss_sum), end='')\n",
    "        \n",
    "        # 测试\n",
    "        correct = 0\n",
    "        model_cost_sensitive.eval()\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model_cost_sensitive(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        print('epoch: {}, test correct: {}'.format(epoch,correct/len(test_loader.dataset)))\n",
    "        \n",
    "#         correct = 0\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "#             data, sign = my_fgsm(data, target, model_cost_sensitive, criterion_cost_sensitive, epsilon, DEVICE, C)\n",
    "#             output = model_cost_sensitive(data)\n",
    "#             pred = output.argmax(dim=1, keepdim=True)\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "#         print('epoch: {}, test correct on adv: {}'.format(epoch,correct/len(test_loader.dataset)))\n",
    "    \n",
    "#     # 保存模型\n",
    "#     if NORMALIZE:\n",
    "#         model_save_path = './model/LeNet_MNIST_cost_sensitive_extension_' + str(i_label) +'.pt'\n",
    "#     else:\n",
    "#         model_save_path = './model/LeNet_MNIST_unnormalized_cost_sensitive_extension_' + str(i_label) +'.pt'\n",
    "#     torch.save(model_cost_sensitive.state_dict(), model_save_path)\n",
    "    \n",
    "    \n",
    "    # 训练结束\n",
    "#     比较结果\n",
    "\n",
    "    ## 对 model_cost_sensitive 的评估\n",
    "    print('对 model_cost_sensitive 的评估')\n",
    "    results_info = {}\n",
    "    model_cost_sensitive.eval()\n",
    "    images_targets = {}\n",
    "    for special_index in range(10):\n",
    "        count = 0\n",
    "        correct = 0\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data = data[target==special_index]\n",
    "            target = target[target==special_index]\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            data, sign = my_fgsm(data, target, model_cost_sensitive, criterion_cost_sensitive, epsilon, DEVICE, C)\n",
    "            output = model_cost_sensitive(data)\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            count += len(data)\n",
    "            print('\\r {}'.format(count), end='')\n",
    "        images_targets[special_index] = [count, correct/count]\n",
    "        print('\\n {} correct: {}'.format(special_index,correct/count))\n",
    "    results_info[1] = images_targets\n",
    "    \n",
    "    ## 对 model_normal 的评估\n",
    "    print('对 model_normal的评估')\n",
    "    model_normal.eval()\n",
    "    criterion_normal = nn.CrossEntropyLoss()\n",
    "    for special_index in range(10):\n",
    "        count = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            data = data[target==special_index]\n",
    "            target = target[target==special_index]\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            data, sign = my_fgsm(data, target, model_normal, criterion_normal, epsilon, DEVICE)\n",
    "            output = model_normal(data)\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            count += len(data)\n",
    "            print('\\r {}'.format(count), end='')   \n",
    "        print('\\n {} correct: {}'.format(special_index,correct/count))\n",
    "\n",
    "    \n",
    "    ## 对 model_adv 的评估\n",
    "    print('对 model_adv的评估')\n",
    "    model_ADV.eval()\n",
    "    images_targets = {}\n",
    "    criterion_normal = nn.CrossEntropyLoss()\n",
    "    for special_index in range(10):\n",
    "        count = 0\n",
    "        correct = 0\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data = data[target==special_index]\n",
    "            target = target[target==special_index]\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            data, sign = my_fgsm(data, target, model_ADV, criterion_normal, epsilon, DEVICE)\n",
    "            output = model_ADV(data)\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            count += len(data)\n",
    "            print('\\r {}'.format(count), end='')\n",
    "        images_targets[special_index] = [count, correct/count]\n",
    "        print('\\n {} correct: {}'.format(special_index,correct/count))\n",
    "    results_info[2] = images_targets\n",
    "    \n",
    "    # 记录结果\n",
    "    results_infos[i_label] = results_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
